{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Oak-D Aruco Tag Distance Test\n",
    "\n",
    "Use this notebook to test how far away the Oak-d camera can detect an Aruco tag. We've found that a high definition camera (1920x1080) can detect a tag from about 35 feet away, but Oak-d camera has 4k resolution, so hopefully it can sense the tags from a greater distance (the target is 70 feet).\n",
    "\n",
    "## Setup\n",
    "\n",
    "You'll need to set up the DepthAI program (see Adrien's setup steps [here](https://github.com/SJSURoboticsTeam/urc-intelligent_systems-2023/blob/main/Vision/README.md)). You will also need to install OpenCV-Contrib version 4.6.0.66 (a lower version is okay, but a higher has breaking API changes)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python==4.6.0.66"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import glob\n",
    "import depthai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load the 4x4_50 aruco tag dictionary\n",
    "arucoDict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "parameters = cv2.aruco.DetectorParameters_create()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Camera\n",
    "\n",
    "Here is where we define the camera. NOTE: this code has been modified since the last time I had access to an Oak-D camera, so it hasn't been tested. There may be some subtle semantics error that I don't know about, so if you run into issues either google it or contact me. The important thing is getting the camera initialized for 4k video (not preview, which is more suited for smaller resolutions)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Pipeline tells DepthAI what operations to perform when running - you define all of the resources used and flows here\n",
    "pipeline = depthai.Pipeline()\n",
    "\n",
    "# First, we want the Color camera as the output\n",
    "cam_rgb = pipeline.createColorCamera()\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.set_Resolution(depthai.ColorCameraProperties.SensorResolution.THE_4_K)\n",
    "cam_rgb.setVideoSize(4056, 3040)\n",
    "\n",
    "# XLinkOut is a \"way out\" from the device. Any data you want to transfer to host need to be send via XLink\n",
    "xout_rgb = pipeline.createXLinkOut()\n",
    "# For the rgb camera output, we want the XLink stream to be named \"rgb\"\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "# Linking camera preview to XLink input, so that the frames will be sent to host\n",
    "cam_rgb.video.link(xout_rgb.input)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "!!IMPORTANT!! Before going any further, make sure that the frame is actually a 4k image. If it's not, then play around with things (following this tutorial here: https://docs.luxonis.com/projects/api/en/latest/tutorials/dispaying_detections/) until it works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with depthai.Device(pipeline) as device:\n",
    "    frame = device.getOutputQueue(\"rgb\").get().getCvFrame()\n",
    "    print(frame.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detecting Aruco Tags\n",
    "\n",
    "This is the main loop where we try to find aruco tags. It'll draw a green bounding box around any tag it detects. It also looks through the last 10 frames to see if it can detect a tag in any of those frames. This helps its accuracy at long distances because the odds of it detecting a tag in single frame isn't great, but over many frames, it's pretty decent. Press 'q' to stop the loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ColorCamera(0) - 'preview' width or height (4056, 3040) bigger than sensor resolution (1920, 1080)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m frame_queue \u001B[38;5;241m=\u001B[39m deque(maxlen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mdepthai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDevice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m device:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# From this point, the Device will be in \"running\" mode and will start sending data via XLink\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# To consume the device results, we get two output queues from the device, with stream names we assigned earlier\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     q_rgb \u001B[38;5;241m=\u001B[39m device\u001B[38;5;241m.\u001B[39mgetOutputQueue(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m     frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: ColorCamera(0) - 'preview' width or height (4056, 3040) bigger than sensor resolution (1920, 1080)"
     ]
    }
   ],
   "source": [
    "frame_queue = deque(maxlen=10)\n",
    "\n",
    "with depthai.Device(pipeline) as device:\n",
    "    # From this point, the Device will be in \"running\" mode and will start sending data via XLink\n",
    "\n",
    "    # To consume the device results, we get two output queues from the device, with stream names we assigned earlier\n",
    "    q_rgb = device.getOutputQueue(\"rgb\")\n",
    "\n",
    "    frame = None\n",
    "\n",
    "    while True:\n",
    "        in_rgb = q_rgb.tryGet()\n",
    "\n",
    "        if in_rgb is not None:\n",
    "            # If the packet from RGB camera is present, we're retrieving the frame in OpenCV format using getCvFrame\n",
    "            frame = in_rgb.getCvFrame()\n",
    "            print(frame.shape)\n",
    "            frame_queue.append(frame)\n",
    "\n",
    "        corners, ids = [], []\n",
    "\n",
    "        for test_frame in frame_queue:\n",
    "            # detect the aruco markers in the image\n",
    "            possible_corners, possible_ids, rejectedImgPoints = cv2.aruco.detectMarkers(test_frame, arucoDict,parameters=parameters)\n",
    "            if len(possible_corners) > 0:\n",
    "                corners.extend(possible_corners)\n",
    "                ids.append(possible_ids.flatten())\n",
    "                break\n",
    "\n",
    "        for corner, tag_id in zip(corners, ids):\n",
    "            # loop over the detected ArUCo corners\n",
    "            #for (markerCorner, markerID) in zip(corners, ids):\n",
    "            # extract the marker corners (which are always returned in\n",
    "            # top-left, top-right, bottom-right, and bottom-left order)\n",
    "            # corners = markerCorner.reshape((4, 2))\n",
    "            tag_id = tag_id[0]\n",
    "\n",
    "            (topLeft, topRight, bottomRight, bottomLeft) = corner[0]\n",
    "            # convert each of the (x, y)-coordinate pairs to integers\n",
    "            topRight = (int(topRight[0]), int(topRight[1]))\n",
    "            bottomRight = (int(bottomRight[0]), int(bottomRight[1]))\n",
    "            bottomLeft = (int(bottomLeft[0]), int(bottomLeft[1]))\n",
    "            topLeft = (int(topLeft[0]), int(topLeft[1]))\n",
    "            # draw the bounding box of the ArUCo detection\n",
    "            cv2.line(frame, topLeft, topRight, (0, 255, 0), 4)\n",
    "            cv2.line(frame, topRight, bottomRight, (0, 255, 0), 4)\n",
    "            cv2.line(frame, bottomRight, bottomLeft, (0, 255, 0), 4)\n",
    "            cv2.line(frame, bottomLeft, topLeft, (0, 255, 0), 4)\n",
    "            # compute and draw the center (x, y)-coordinates of the ArUco\n",
    "            # marker\n",
    "            cX = int((topLeft[0] + bottomRight[0]) / 2.0)\n",
    "            cY = int((topLeft[1] + bottomRight[1]) / 2.0)\n",
    "            cv2.circle(frame, (cX, cY), 4, (0, 0, 255), -1)\n",
    "            # draw the ArUco marker ID on the frame\n",
    "            cv2.putText(frame, str(tag_id),\n",
    "                (topLeft[0], topLeft[1] - 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if frame is not None:\n",
    "            cv2.resize(frame, None, fx=.25, fy=.25)\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            #if the `q` key was pressed, break from the loop\n",
    "            if key == ord(\"q\"):\n",
    "               break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
